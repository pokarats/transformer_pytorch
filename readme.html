
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Build Your Own Transformer 1.0 documentation</title>
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="_static/language_data.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="build-your-own-transformer-s-documentation">
<h1>Build Your Own Transformer’s documentation!<a class="headerlink" href="#build-your-own-transformer-s-documentation" title="Permalink to this headline">¶</a></h1>
<p>Implementation of the Transformer model. The submission also includes wrapper code for using the model for training and
testing in a translation.</p>
<p>For data, I used the provided dataset, specifically the <code class="docutils literal notranslate"><span class="pre">news-commentary-v8.de-en.de</span></code> and
<code class="docutils literal notranslate"><span class="pre">news-commentary-v8.de-en.en</span></code>.</p>
<p>For time efficiency, I have only trained the model on the first 1024 sentences partition of the dataset to make sure
that the model executes without errors. The results from simply running this small toy dataset does not seem to work as
it shows a BLEU score of 0.</p>
<p>I have not fully trained and evaluated the model on the entire dataset or on other datasets
to verify how well this works. However, from the <code class="docutils literal notranslate"><span class="pre">train_model.log</span></code> file, it does show that the training and validation
losses improve after a few epochs, which seems to be expected.</p>
<div class="section" id="project-file-structure">
<h2>Project File Structure<a class="headerlink" href="#project-file-structure" title="Permalink to this headline">¶</a></h2>
<dl class="simple">
<dt>The project is organized inside the <em>transformer</em> directory as follows:</dt><dd><ul class="simple">
<li><dl class="simple">
<dt>data: <em>training data files are inside this directory</em></dt><dd><ul>
<li><p>news-commentary-v8.de-en.de</p></li>
<li><p>news-commentary-v8.de-en.en</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>log: <em>logging files from running train)model.py and eval_model.py</em></dt><dd><ul>
<li><p>eval_model.log</p></li>
<li><p>train_model.log</p></li>
</ul>
</dd>
</dl>
</li>
<li><p>pkl: <em>by default, script will save pickle files here; script will create this directory if it doesn’t exist</em></p></li>
<li><p>saved_models: <em>by default, script will save model checkpoints here; script will create this directory if it doesn’t exist</em></p></li>
<li><p>eval_model.py</p></li>
<li><p>preprocess.py</p></li>
<li><p>train_model.py</p></li>
<li><p>transformer_model.py</p></li>
<li><p>utils.py</p></li>
</ul>
</dd>
</dl>
</div>
<div class="section" id="required-libraries">
<h2>Required Libraries<a class="headerlink" href="#required-libraries" title="Permalink to this headline">¶</a></h2>
<dl class="simple">
<dt>In order to run the code, the following modules need to be installed first:</dt><dd><ul class="simple">
<li><p>PyTorch</p></li>
<li><p>TorchText</p></li>
<li><dl class="simple">
<dt>spacy and ‘de’ and ‘en’ language models:</dt><dd><ul>
<li><p>python -m spacy download en</p></li>
<li><p>python -m spacy download de</p></li>
</ul>
</dd>
</dl>
</li>
<li><p>dill</p></li>
</ul>
</dd>
</dl>
<p>Program is written and tested in Python 3.7.</p>
</div>
<div class="section" id="run-instructions">
<h2>Run Instructions<a class="headerlink" href="#run-instructions" title="Permalink to this headline">¶</a></h2>
<p>The root level for this project is inside the <em>transformer</em> directory.</p>
<p><code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">train_model.py</span></code> will train the model with default hyper-parameters and save the best model weights.</p>
<dl class="simple">
<dt>usage: train_model.py [-h] [-data_path DATA_PATH] [-log_path LOG_PATH]</dt><dd><p>[-pkl_path PKL_PATH] [-model_path MODEL_PATH]
[-src_data SRC_DATA] [-trg_data TRG_DATA]
[-src_lang SRC_LANG] [-trg_lang TRG_LANG]
[-epochs EPOCHS] [-dmodel DMODEL] [-dff DFF]
[-nlayers NLAYERS] [-heads HEADS] [-dropout DROPOUT]
[-batch_size BATCH_SIZE] [-lr LR] [-max_len MAX_LEN]
[-num_sents NUM_SENTS] [-toy_run TOY_RUN] [-debug DEBUG]
[-save_model SAVE_MODEL] [-override OVERRIDE]
[-seed SEED]</p>
</dd>
</dl>
<p>optional arguments:</p>
<blockquote>
<div><dl class="option-list">
<dt><kbd><span class="option">-h</span></kbd></dt>
<dd><p>show this help message and exit</p>
</dd>
<dt><kbd><span class="option">-d<var>ata_path</var></span></kbd></dt>
<dd><p>data directory; default=’data’</p>
</dd>
<dt><kbd><span class="option">-l<var>og_path</var></span></kbd></dt>
<dd><p>log file directory; default=’log’,</p>
</dd>
<dt><kbd><span class="option">-p<var>kl_path</var></span></kbd></dt>
<dd><p>pickle file directory; default=’pkl’</p>
</dd>
<dt><kbd><span class="option">-m<var>odel_path</var></span></kbd></dt>
<dd><p>saved models directory; default=’saved_models’</p>
</dd>
<dt><kbd><span class="option">-s<var>rc_data</var></span></kbd></dt>
<dd><p>src corpus filename; default=’news-commentary-v8.de-en.de’</p>
</dd>
<dt><kbd><span class="option">-t<var>rg_data</var></span></kbd></dt>
<dd><p>trg corpus filename; default=’news-commentary-v8.de-en.en’</p>
</dd>
<dt><kbd><span class="option">-s<var>rc_lang</var></span></kbd></dt>
<dd><p>source language; default=’de’</p>
</dd>
<dt><kbd><span class="option">-t<var>rg_lang</var></span></kbd></dt>
<dd><p>target language; default=’en’</p>
</dd>
<dt><kbd><span class="option">-e<var>pochs</var></span></kbd></dt>
<dd><p>number of epochs to train for; default=25</p>
</dd>
<dt><kbd><span class="option">-d<var>model</var></span></kbd></dt>
<dd><p>d_model or hidden size; default=512</p>
</dd>
<dt><kbd><span class="option">-d<var>ff</var></span></kbd></dt>
<dd><p>d_ff or hidden size of FFN sublayer; default=2048</p>
</dd>
<dt><kbd><span class="option">-n<var>layers</var></span></kbd></dt>
<dd><p>number of encoder/decoder layers; default=6</p>
</dd>
<dt><kbd><span class="option">-h<var>eads</var></span></kbd></dt>
<dd><p>number of attention heads; default=8</p>
</dd>
<dt><kbd><span class="option">-d<var>ropout</var></span></kbd></dt>
<dd><p>value for dropout p parameter; default=0.1</p>
</dd>
<dt><kbd><span class="option">-b<var>atch_size</var></span></kbd></dt>
<dd><p>number of samples per batch; default=48</p>
</dd>
<dt><kbd><span class="option">-l<var>r</var></span></kbd></dt>
<dd><p>learning rate for gradient update; default=3e-4</p>
</dd>
<dt><kbd><span class="option">-m<var>ax_len</var></span></kbd></dt>
<dd><p>maximum number of tokens in a sentence; default=150</p>
</dd>
<dt><kbd><span class="option">-n<var>um_sents</var></span></kbd></dt>
<dd><p>number of sentences to partition toy corpus; default=1024</p>
</dd>
<dt><kbd><span class="option">-t<var>oy_run</var></span></kbd></dt>
<dd><p>whether or not toy dataset; default=False</p>
</dd>
<dt><kbd><span class="option">-d<var>ebug</var></span></kbd></dt>
<dd><p>turn logging to debug mode to display more info; default=False</p>
</dd>
<dt><kbd><span class="option">-s<var>ave_model</var></span></kbd></dt>
<dd><p>True to save model checkpoint; default=True</p>
</dd>
<dt><kbd><span class="option">-o<var>verride</var></span></kbd></dt>
<dd><p>override existing log file; default=False</p>
</dd>
<dt><kbd><span class="option">-s<var>eed</var></span></kbd></dt>
<dd><p>seed for the iterator random shuffling repeat; default=1234</p>
</dd>
</dl>
</div></blockquote>
<p><code class="docutils literal notranslate"><span class="pre">train_model.py</span></code> has to be executed before eval_model.py.</p>
<p><code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">eval_model.py</span></code> will evaluate the model on the test data and calculate BLEU score.</p>
<dl>
<dt>usage: eval_model.py [-h] [-data_path DATA_PATH] [-log_path LOG_PATH]</dt><dd><p>[-pkl_path PKL_PATH] [-model_path MODEL_PATH]
[-src_data SRC_DATA] [-trg_data TRG_DATA]
[-src_lang SRC_LANG] [-trg_lang TRG_LANG]
[-dmodel DMODEL] [-dff DFF] [-nlayers NLAYERS]
[-heads HEADS] [-dropout DROPOUT]
[-batch_size BATCH_SIZE] [-lr LR] [-max_len MAX_LEN]
[-num_sents NUM_SENTS] [-toy TOY] [-override OVERRIDE]
[-seed SEED] [-sample_idx SAMPLE_IDX]</p>
</dd>
<dt>optional arguments:</dt><dd><dl class="option-list">
<dt><kbd><span class="option">-h</span></kbd></dt>
<dd><p>show this help message and exit</p>
</dd>
<dt><kbd><span class="option">-d<var>ata_path</var></span></kbd></dt>
<dd><p>data directory; default=’data’</p>
</dd>
<dt><kbd><span class="option">-l<var>og_path</var></span></kbd></dt>
<dd><p>log file directory; default=’log’</p>
</dd>
<dt><kbd><span class="option">-p<var>kl_path</var></span></kbd></dt>
<dd><p>pickle file directory; default=’pkl’</p>
</dd>
<dt><kbd><span class="option">-m<var>odel_path</var></span></kbd></dt>
<dd><p>saved models directory; default=’saved_models’</p>
</dd>
<dt><kbd><span class="option">-s<var>rc_data</var></span></kbd></dt>
<dd><p>src corpus filename; default=’news-commentary-v8.de-en.de’</p>
</dd>
<dt><kbd><span class="option">-t<var>rg_data</var></span></kbd></dt>
<dd><p>trg corpus filename; default=’news-commentary-v8.de-en.en’</p>
</dd>
<dt><kbd><span class="option">-s<var>rc_lang</var></span></kbd></dt>
<dd><p>source language; default=’de’</p>
</dd>
<dt><kbd><span class="option">-t<var>rg_lang</var></span></kbd></dt>
<dd><p>target language; default=’en’</p>
</dd>
<dt><kbd><span class="option">-d<var>model</var></span></kbd></dt>
<dd><p>d_model or hidden size; default=512</p>
</dd>
<dt><kbd><span class="option">-d<var>ff</var></span></kbd></dt>
<dd><p>d_ff or hidden size of FFN sublayer; default=2048</p>
</dd>
<dt><kbd><span class="option">-n<var>layers</var></span></kbd></dt>
<dd><p>number of encoder/decoder layers; default=6</p>
</dd>
<dt><kbd><span class="option">-h<var>eads</var></span></kbd></dt>
<dd><p>number of attention heads; default=8</p>
</dd>
<dt><kbd><span class="option">-d<var>ropout</var></span></kbd></dt>
<dd><p>value for dropout p parameter; default=0.1</p>
</dd>
<dt><kbd><span class="option">-b<var>atch_size</var></span></kbd></dt>
<dd><p>number of samples per batch; default=48</p>
</dd>
<dt><kbd><span class="option">-l<var>r</var></span></kbd></dt>
<dd><p>learning rate for gradient update; default=3e-4</p>
</dd>
<dt><kbd><span class="option">-m<var>ax_len</var></span></kbd></dt>
<dd><p>maximum number of tokens in a sentence; default=150</p>
</dd>
<dt><kbd><span class="option">-n<var>um_sents</var></span></kbd></dt>
<dd><p>number of sentences to partition toy corpus; default=1024</p>
</dd>
<dt><kbd><span class="option">-t<var>oy_run</var></span></kbd></dt>
<dd><p>whether or not toy dataset; default=False</p>
</dd>
<dt><kbd><span class="option">-d<var>ebug</var></span></kbd></dt>
<dd><p>turn logging to debug mode to display more info; default=False</p>
</dd>
<dt><kbd><span class="option">-s<var>ave_model</var></span></kbd></dt>
<dd><p>True to save model checkpoint; default=True</p>
</dd>
<dt><kbd><span class="option">-o<var>verride</var></span></kbd></dt>
<dd><p>override existing log file; default=False</p>
</dd>
<dt><kbd><span class="option">-s<var>eed</var></span></kbd></dt>
<dd><p>seed for the iterator random shuffling repeat; default=1234</p>
</dd>
<dt><kbd><span class="option">-s<var>ample_idx</var></span></kbd></dt>
<dd><p>index for a sample sentence pair example; default=8</p>
</dd>
</dl>
</dd>
</dl>
<p>Results are recorded in the log files.</p>
<div class="toctree-wrapper compound">
</div>
</div>
</div>
<div class="section" id="modules">
<h1>Modules<a class="headerlink" href="#modules" title="Permalink to this headline">¶</a></h1>
<div class="section" id="module-preprocess">
<span id="preprocessing"></span><h2>Preprocessing<a class="headerlink" href="#module-preprocess" title="Permalink to this headline">¶</a></h2>
<p><strong>preprocess.py Module</strong></p>
<p>Script to pre-process data files into Transformer compatible batches and batch iterators</p>
<dl class="class">
<dt id="preprocess.Vocabulary">
<em class="property">class </em><code class="sig-prename descclassname">preprocess.</code><code class="sig-name descname">Vocabulary</code><span class="sig-paren">(</span><em class="sig-param">src_path</em>, <em class="sig-param">trg_path</em>, <em class="sig-param">tokenizer='spacy'</em>, <em class="sig-param">src_lang='de'</em>, <em class="sig-param">trg_lang='en'</em><span class="sig-paren">)</span><a class="headerlink" href="#preprocess.Vocabulary" title="Permalink to this definition">¶</a></dt>
<dd><p>Class to generate pre-processed dataset into batches and interators for training, validation, testing sets.
Module uses torchtext to generate dataset and iterators.
The default tokenizer is ‘spacy’</p>
<dl class="method">
<dt id="preprocess.Vocabulary.make_batch_iterators">
<em class="property">static </em><code class="sig-name descname">make_batch_iterators</code><span class="sig-paren">(</span><em class="sig-param">train_data</em>, <em class="sig-param">val_data</em>, <em class="sig-param">test_data</em>, <em class="sig-param">batch_size</em>, <em class="sig-param">device</em><span class="sig-paren">)</span><a class="headerlink" href="#preprocess.Vocabulary.make_batch_iterators" title="Permalink to this definition">¶</a></dt>
<dd><p>Create batch iterators for training, validation, and testing datasets using torchtext.data.BucketIterator</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>train_data</strong> – training set in TabularDataset object</p></li>
<li><p><strong>val_data</strong> – validation set in TabularDataset object</p></li>
<li><p><strong>test_data</strong> – test set in TabularDataset object</p></li>
<li><p><strong>batch_size</strong> – number of sentences in a batch</p></li>
<li><p><strong>device</strong> – cuda or cpu</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="preprocess.Vocabulary.make_datasets">
<code class="sig-name descname">make_datasets</code><span class="sig-paren">(</span><em class="sig-param">data_path</em>, <em class="sig-param">train_file</em>, <em class="sig-param">val_file</em>, <em class="sig-param">test_file</em>, <em class="sig-param">max_len</em>, <em class="sig-param">pkl_path</em>, <em class="sig-param">src_pkl_file='de_Field.pkl'</em>, <em class="sig-param">trg_pkl_file='en_Field.pkl'</em>, <em class="sig-param">sos='&lt;sos&gt;'</em>, <em class="sig-param">eos='&lt;eos&gt;'</em>, <em class="sig-param">saved_field=False</em><span class="sig-paren">)</span><a class="headerlink" href="#preprocess.Vocabulary.make_datasets" title="Permalink to this definition">¶</a></dt>
<dd><p>Create training, validation, and test datasets from json files in TabularDataset format.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data_path</strong> – path to data directory</p></li>
<li><p><strong>train_file</strong> – training json file (should be the same as the file saved in to_json function)</p></li>
<li><p><strong>val_file</strong> – validation json file (should be the same as the file saved in to_json function)</p></li>
<li><p><strong>test_file</strong> – test json file (should be the same as the file saved in to_json function)</p></li>
<li><p><strong>max_len</strong> – maximum number of tokens in the sentence</p></li>
<li><p><strong>pkl_path</strong> – path to pickle file directory</p></li>
<li><p><strong>src_pkl_file</strong> – src pickle filename</p></li>
<li><p><strong>trg_pkl_file</strong> – trg pickle filename</p></li>
<li><p><strong>sos</strong> – start of sentence token</p></li>
<li><p><strong>eos</strong> – end of sentence token</p></li>
<li><p><strong>saved_field</strong> – whether or not there’s a saved torchtext.data.Field pickled file to be loaded</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="preprocess.Vocabulary.make_train_val_test_splits">
<code class="sig-name descname">make_train_val_test_splits</code><span class="sig-paren">(</span><em class="sig-param">max_len</em>, <em class="sig-param">max_diff</em>, <em class="sig-param">test_split_size=0.2</em><span class="sig-paren">)</span><a class="headerlink" href="#preprocess.Vocabulary.make_train_val_test_splits" title="Permalink to this definition">¶</a></dt>
<dd><p>Create training, validation, and test splits</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>max_len</strong> – maximum number of tokens in the sentence</p></li>
<li><p><strong>max_diff</strong> – maximum factor of difference in number of tokens between src and trg sentences</p></li>
<li><p><strong>test_split_size</strong> – proportion to hold out for test set</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="preprocess.Vocabulary.partition_raw_data">
<code class="sig-name descname">partition_raw_data</code><span class="sig-paren">(</span><em class="sig-param">num_sents</em><span class="sig-paren">)</span><a class="headerlink" href="#preprocess.Vocabulary.partition_raw_data" title="Permalink to this definition">¶</a></dt>
<dd><p>Partition dataset to specified number o sentences to create toy dataset</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>num_sents</strong> – specify number of sentences to use for toy dataset</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="preprocess.Vocabulary.to_json">
<em class="property">static </em><code class="sig-name descname">to_json</code><span class="sig-paren">(</span><em class="sig-param">data_frame</em>, <em class="sig-param">path</em>, <em class="sig-param">filename</em><span class="sig-paren">)</span><a class="headerlink" href="#preprocess.Vocabulary.to_json" title="Permalink to this definition">¶</a></dt>
<dd><p>Save data farm to json</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data_frame</strong> – pandas dataframe</p></li>
<li><p><strong>path</strong> – path to data directory</p></li>
<li><p><strong>filename</strong> – filename for the .json file</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-train_model">
<span id="training"></span><h2>Training<a class="headerlink" href="#module-train_model" title="Permalink to this headline">¶</a></h2>
<p><strong>train_model.py Module</strong></p>
<p>Wrapper module to train Transformer model and save model check points based on best validation loss.</p>
<p>This is the module to run to train the model. Model parameters can be configured/changed from default values
via command line options. Otherwise, default values are as per the “Attention is All You Need” paper:</p>
<p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., &amp; Polosukhin, I. (2017).
Attention Is All You Need. ArXiv:1706.03762 [Cs]. http://arxiv.org/abs/1706.03762</p>
<dl class="function">
<dt id="train_model.epoch_time">
<code class="sig-prename descclassname">train_model.</code><code class="sig-name descname">epoch_time</code><span class="sig-paren">(</span><em class="sig-param">start_time: float</em>, <em class="sig-param">end_time: float</em><span class="sig-paren">)</span><a class="headerlink" href="#train_model.epoch_time" title="Permalink to this definition">¶</a></dt>
<dd><p>Function to calculate elapsed time in mins and secs</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>start_time</strong> – time function ended</p></li>
<li><p><strong>end_time</strong> – time function finished</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>elapsed minutes, elapsed seconds</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="train_model.evaluate">
<code class="sig-prename descclassname">train_model.</code><code class="sig-name descname">evaluate</code><span class="sig-paren">(</span><em class="sig-param">model</em>, <em class="sig-param">iterator</em>, <em class="sig-param">criterion</em>, <em class="sig-param">device</em><span class="sig-paren">)</span><a class="headerlink" href="#train_model.evaluate" title="Permalink to this definition">¶</a></dt>
<dd><p>run model in eval mode in batches</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> – model to be used for evaluation, e.g. Transformer model</p></li>
<li><p><strong>iterator</strong> – torchtext.data.BucketIterator object</p></li>
<li><p><strong>criterion</strong> – loss function</p></li>
<li><p><strong>device</strong> – cpu or cuda</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>epoch validation or test loss</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="train_model.train">
<code class="sig-prename descclassname">train_model.</code><code class="sig-name descname">train</code><span class="sig-paren">(</span><em class="sig-param">model</em>, <em class="sig-param">iterator</em>, <em class="sig-param">optimizer</em>, <em class="sig-param">criterion</em>, <em class="sig-param">clip</em>, <em class="sig-param">device</em><span class="sig-paren">)</span><a class="headerlink" href="#train_model.train" title="Permalink to this definition">¶</a></dt>
<dd><p>Function to train in batches</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> – model to be used for training, e.g. Transformer model</p></li>
<li><p><strong>iterator</strong> – torchtext.data.BucketIterator object</p></li>
<li><p><strong>optimizer</strong> – torch optimizer</p></li>
<li><p><strong>criterion</strong> – loss function</p></li>
<li><p><strong>clip</strong> – parameter for clip_grad_norm</p></li>
<li><p><strong>device</strong> – cpu or cuda</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>epoch loss</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-transformer_model">
<span id="the-transformer-model"></span><h2>The Transformer Model<a class="headerlink" href="#module-transformer_model" title="Permalink to this headline">¶</a></h2>
<p><strong>transformer_model.py Module</strong></p>
<p>Implementation of the Transformer model as described in the ‘Attention is All You Need’ paper:
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., &amp; Polosukhin, I. (2017).
Attention Is All You Need. ArXiv:1706.03762 [Cs]. http://arxiv.org/abs/1706.03762</p>
<dl class="simple">
<dt>I followed the implementation examples in the following resources:</dt><dd><ul class="simple">
<li><p>“The Annotated Transformer: <a class="reference external" href="https://nlp.seas.harvard.edu/2018/04/03/attention.html">https://nlp.seas.harvard.edu/2018/04/03/attention.html</a>”</p></li>
<li><p>“Aladdin Perssons’ Transformer from Scratch YouTube video: <a class="reference external" href="https://www.youtube.com/watch?v=U0s0f995w14&amp;ab_channel=AladdinPersson">https://www.youtube.com/watch?v=U0s0f995w14&amp;ab_channel=AladdinPersson</a>”</p></li>
<li><p>“Ben Trevett’s Language Translation with Transformer and Torchtext tutorial: <a class="reference external" href="https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html?highlight=transformer">https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html?highlight=transformer</a>”</p></li>
</ul>
</dd>
</dl>
<p>I tried to balance between adhering to the variable names in the paper and using plain English for ease of comprehension</p>
<dl class="class">
<dt id="transformer_model.Decoder">
<em class="property">class </em><code class="sig-prename descclassname">transformer_model.</code><code class="sig-name descname">Decoder</code><span class="sig-paren">(</span><em class="sig-param">trg_vocab_size</em>, <em class="sig-param">d_model</em>, <em class="sig-param">nx_layers</em>, <em class="sig-param">n_heads</em>, <em class="sig-param">d_ff</em>, <em class="sig-param">dropout_p</em>, <em class="sig-param">max_length</em>, <em class="sig-param">device</em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_model.Decoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Implement the Decoder block of nx DecodingLayers. From input token embeddings + position embeddings and
Nx DecodingLayers to generating the output that will be interpreted as the predicted translation</p>
<dl class="method">
<dt id="transformer_model.Decoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">trg: torch.Tensor</em>, <em class="sig-param">src_encoder_output: torch.Tensor</em>, <em class="sig-param">trg_mask: torch.Tensor</em>, <em class="sig-param">src_mask: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="headerlink" href="#transformer_model.Decoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>trg</strong> – src shape (N, trg_seq_len)</p></li>
<li><p><strong>src_encoder_output</strong> – src_input_embeddings shape (N, src_seq_len, d_model)</p></li>
<li><p><strong>trg_mask</strong> – shape (batch N, 1, trg_seq_len, trg_seq_len)</p></li>
<li><p><strong>src_mask</strong> – shape (N, 1, 1, src_seq_len)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>trg decoder ouput shape: (N, trg_seq_len, d_model)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="transformer_model.DecoderLayer">
<em class="property">class </em><code class="sig-prename descclassname">transformer_model.</code><code class="sig-name descname">DecoderLayer</code><span class="sig-paren">(</span><em class="sig-param">d_model</em>, <em class="sig-param">n_heads</em>, <em class="sig-param">d_ff</em>, <em class="sig-param">dropout_p</em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_model.DecoderLayer" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Implement the Decoder block with the 3 sublayers:</dt><dd><ol class="arabic simple">
<li><p>decoder attention sublayer</p></li>
<li><p>encoder attention sublayer</p></li>
<li><p>FFN sublayer</p></li>
</ol>
</dd>
</dl>
<p>This layer is stacked n_layers times in the Decoder part of the Transformer architecture</p>
<dl class="method">
<dt id="transformer_model.DecoderLayer.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">trg</em>, <em class="sig-param">src_encoder_output</em>, <em class="sig-param">trg_mask</em>, <em class="sig-param">src_mask</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="headerlink" href="#transformer_model.DecoderLayer.forward" title="Permalink to this definition">¶</a></dt>
<dd><ol class="arabic simple">
<li><dl class="simple">
<dt>masked decoder attention sublayer</dt><dd><p>1.a in the EncoderLayer V,K,Q are all from the same input trg</p>
</dd>
</dl>
</li>
<li><p>add and normalize attention sublayer output with residual input from before the decoder attention sublayer</p></li>
<li><p>apply dropout to the added and normed output of the attention sublayer</p></li>
<li><dl class="simple">
<dt>encoder attention sublayer</dt><dd><p>4.a in the DecoderLayer Q is from output of previous decoder sublayer,
V,K are from last encoder layer output</p>
</dd>
</dl>
</li>
<li><p>add and normalize encoder attention sublayer output with residual from the output of step 3.</p></li>
<li><p>apply dropout to the added and normed output of the encoder attention sublayer</p></li>
<li><p>output from encoder attention sublayer goes through FFN</p></li>
<li><p>add and normalize output_from_ffn with residual (output from decoder attention sublayer) from before the ffn layer</p></li>
<li><p>apply dropout to the added and normed output of the FFN sublayer</p></li>
</ol>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>trg</strong> – shape (N, trg_seq_len, d_model)</p></li>
<li><p><strong>src_encoder_output</strong> – shape (N, src_seq_len, d_model)</p></li>
<li><p><strong>trg_mask</strong> – shape (batch N, 1, trg_seq_len, trg_seq_len)</p></li>
<li><p><strong>src_mask</strong> – shape (N, 1, 1, src_seq_len)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="transformer_model.Encoder">
<em class="property">class </em><code class="sig-prename descclassname">transformer_model.</code><code class="sig-name descname">Encoder</code><span class="sig-paren">(</span><em class="sig-param">src_vocab_size</em>, <em class="sig-param">d_model</em>, <em class="sig-param">nx_layers</em>, <em class="sig-param">n_heads</em>, <em class="sig-param">d_ff</em>, <em class="sig-param">dropout_p</em>, <em class="sig-param">max_length</em>, <em class="sig-param">device</em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_model.Encoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Implement the Encoder block of nx Encoder layers. From input token embeddings + position embeddings and
Nx EncodingLayers to generating the output that will be taken up by the Decoder block</p>
<dl class="method">
<dt id="transformer_model.Encoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">src: torch.Tensor</em>, <em class="sig-param">src_mask: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="headerlink" href="#transformer_model.Encoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>src</strong> – src shape (N, src_seq_len)</p></li>
<li><p><strong>src_mask</strong> – src_mask shape (N, 1, 1, src_seq_len)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>output from the last encoder layer in the stack</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="transformer_model.EncoderLayer">
<em class="property">class </em><code class="sig-prename descclassname">transformer_model.</code><code class="sig-name descname">EncoderLayer</code><span class="sig-paren">(</span><em class="sig-param">d_model</em>, <em class="sig-param">n_heads</em>, <em class="sig-param">d_ff</em>, <em class="sig-param">dropout_p</em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_model.EncoderLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Implement the Encoder block with the 2 sublayers: 1) attention sublayer 2) FFN sublayer.
This layer is stacked n_layers times in the Encoder part of the Transformer architecture</p>
<dl class="method">
<dt id="transformer_model.EncoderLayer.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">src</em>, <em class="sig-param">src_mask</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="headerlink" href="#transformer_model.EncoderLayer.forward" title="Permalink to this definition">¶</a></dt>
<dd><ol class="arabic simple">
<li><dl class="simple">
<dt>attention sublayer</dt><dd><p>1.a in the EncoderLayer V,K,Q are all from the same x input</p>
</dd>
</dl>
</li>
<li><p>add and normalize attention sublayer output with residual input from before the attention_sublayer</p></li>
<li><p>apply dropout to the added and normed output of the attention sublayer</p></li>
<li><p>output from attention sublayer goes through FFN</p></li>
<li><p>add and normalize output_from_ffn with residual (output from attention sublayer) from before the ffn layer</p></li>
<li><p>apply dropout to the added and normed output of the FFN sublayer</p></li>
</ol>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>src</strong> – src shape (N, src_seq_len, d_model)</p></li>
<li><p><strong>src_mask</strong> – src_mask shape (N, 1, 1, src_seq_len)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>output Tensor that will become input Tensor in the next EncoderLayer</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="transformer_model.FeedForward">
<em class="property">class </em><code class="sig-prename descclassname">transformer_model.</code><code class="sig-name descname">FeedForward</code><span class="sig-paren">(</span><em class="sig-param">d_model</em>, <em class="sig-param">d_ff</em>, <em class="sig-param">dropout_p</em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_model.FeedForward" title="Permalink to this definition">¶</a></dt>
<dd><p>Implement the FeedForward sublayer in the TransformerBlock:
FFN = Relu(input_x * W_1 + b_1) * W_2 + b_2</p>
<dl class="method">
<dt id="transformer_model.FeedForward.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">output_from_attention_sublayer: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="headerlink" href="#transformer_model.FeedForward.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="transformer_model.Generator">
<em class="property">class </em><code class="sig-prename descclassname">transformer_model.</code><code class="sig-name descname">Generator</code><span class="sig-paren">(</span><em class="sig-param">d_model</em>, <em class="sig-param">trg_vocab_size</em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_model.Generator" title="Permalink to this definition">¶</a></dt>
<dd><p>Implement the last linear layer and the softmax of the Transformer architecture</p>
<p>This is not necessary if using nn.CrossEntropyLoss as softmax is already built-in</p>
<dl class="method">
<dt id="transformer_model.Generator.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">output_from_decoder</em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_model.Generator.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>output_from_decoder</strong> – shape (N, trg_seq_len, d_model)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>output_from_decoder: shape (N, trg_seq_len, trg_vocab_size)???</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="transformer_model.MultiHeadedAttention">
<em class="property">class </em><code class="sig-prename descclassname">transformer_model.</code><code class="sig-name descname">MultiHeadedAttention</code><span class="sig-paren">(</span><em class="sig-param">n_heads: int</em>, <em class="sig-param">d_model: int</em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_model.MultiHeadedAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Implementation of the Multi-headed Attention sublayer in the Transformer block
This sublayer is the same architecture in both the Encoder and Decoder blocks
The multi-headded attention sublayer comprises the scaled-dot product attention mechanism</p>
<dl class="method">
<dt id="transformer_model.MultiHeadedAttention.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">value: torch.Tensor</em>, <em class="sig-param">key: torch.Tensor</em>, <em class="sig-param">query: torch.Tensor</em>, <em class="sig-param">mask: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="headerlink" href="#transformer_model.MultiHeadedAttention.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>value</strong> – value Tensor shape (N, sequence length, d_model)</p></li>
<li><p><strong>key</strong> – key Tensor shape (N, sequence length, d_model)</p></li>
<li><p><strong>query</strong> – query Tensor shape (N, sequence length, d_model)</p></li>
<li><p><strong>mask</strong> – src or trg masking Tensor shape src(N, 1, 1, src_seq_len) trg(N, 1, trg_seq_len, trg_seq_len)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="transformer_model.PositionEmbedding">
<em class="property">class </em><code class="sig-prename descclassname">transformer_model.</code><code class="sig-name descname">PositionEmbedding</code><a class="headerlink" href="#transformer_model.PositionEmbedding" title="Permalink to this definition">¶</a></dt>
<dd><p>TODO implement PositionEmbedding per paper</p>
</dd></dl>

<dl class="class">
<dt id="transformer_model.Transformer">
<em class="property">class </em><code class="sig-prename descclassname">transformer_model.</code><code class="sig-name descname">Transformer</code><span class="sig-paren">(</span><em class="sig-param">src_vocab_size</em>, <em class="sig-param">trg_vocab_size</em>, <em class="sig-param">src_pad_idx</em>, <em class="sig-param">trg_pad_idx</em>, <em class="sig-param">d_model</em>, <em class="sig-param">nx_layers</em>, <em class="sig-param">n_heads</em>, <em class="sig-param">d_ff</em>, <em class="sig-param">dropout_p</em>, <em class="sig-param">max_length</em>, <em class="sig-param">device</em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_model.Transformer" title="Permalink to this definition">¶</a></dt>
<dd><p>Implement the Transformer architecture consisting of nx_layers of the encoder block and nx_layers of the decoder
block.</p>
<dl class="method">
<dt id="transformer_model.Transformer.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">src: torch.Tensor</em>, <em class="sig-param">trg: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="headerlink" href="#transformer_model.Transformer.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>src</strong> – src shape (N, src_seq_len)</p></li>
<li><p><strong>trg</strong> – trg shape (N, trg_seq_len)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>output shape (N, trg_seq_len, d_model)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="transformer_model.Transformer.make_src_mask">
<code class="sig-name descname">make_src_mask</code><span class="sig-paren">(</span><em class="sig-param">src: torch.Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_model.Transformer.make_src_mask" title="Permalink to this definition">¶</a></dt>
<dd><p>Wherever src is not a pad idx, add dim of 1
:param src: src shape (N, src_seq_len)
:return: src_mask shape (N, 1, 1, src_seq_len)</p>
</dd></dl>

<dl class="method">
<dt id="transformer_model.Transformer.make_trg_mask">
<code class="sig-name descname">make_trg_mask</code><span class="sig-paren">(</span><em class="sig-param">trg: torch.Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_model.Transformer.make_trg_mask" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>trg</strong> – trg shape (N, trg_seq_len)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>trg_mask shape (N, 1, trg_seq_len, trg_seq_len)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-eval_model">
<span id="testing"></span><h2>Testing<a class="headerlink" href="#module-eval_model" title="Permalink to this headline">¶</a></h2>
<p><strong>eval_model.py Module</strong></p>
<p>Wrapper function to perform translation with saved model weights after training.
Evaluate model on the test dataset and calculate BLEU score from translation results.</p>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html#document-index">Build Your Own Transformer</a></h1>








<h3>Navigation</h3>
<p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="index.html#document-index">Build Your Own Transformer’s documentation!</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#project-file-structure">Project File Structure</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#required-libraries">Required Libraries</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#run-instructions">Run Instructions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#modules">Modules</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#module-preprocess">Preprocessing</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#module-train_model">Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#module-transformer_model">The Transformer Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#module-eval_model">Testing</a></li>
</ul>
</li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html#document-index">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2021, Noon Pokaratsiri Goldstein.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 2.3.1</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
    </div>

    

    
  </body>
</html>